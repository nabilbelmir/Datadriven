{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabilbelmir/Datadriven/blob/main/solution1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMLyDHE6f71f"
      },
      "source": [
        "# Exercise 1: Principal Component Analysis\n",
        "\n",
        "The goal is to apply PCA to a couple of datasets and to understand the results.\n",
        "The code is written in Python, and it is run on Google Colab.\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "Colab is a service that hosts Jupyter notebooks, which provides the computing resources (also GPUs). You don't need to install anything locally, because Colab provides a remote machine that will run your python code.\n",
        "Jupyter notebooks are files which support both text and code. The code runs in multiple cells, which is very convenient because it let us compartmentalize the script.\n",
        "\n",
        "## Python\n",
        "\n",
        "Python is a (very) high-level programming language, which is interpreted at run-time instead of being compiled. This means that we can run the code without needing to compile it first.\n",
        "Python is very popular in machine learning because it offers a huge number of libraries that we can use to build machine-learning and deep learning models. Also, it is a dynamically typed language, meaning that we do not need to define the type of variable before assignment.\n",
        "\n",
        "### A brief intro to Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXVlNJWdf71i"
      },
      "outputs": [],
      "source": [
        "# create some variables\n",
        "a = 3.2 # this is a float\n",
        "b = 'test' # this is a string\n",
        "c = [1, 2, 3] # this is a list\n",
        "\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOYNTA8Qf71j"
      },
      "outputs": [],
      "source": [
        "# do some math\n",
        "\n",
        "a = 3.2\n",
        "b = 7.4\n",
        "\n",
        "c = a * b # multiplication\n",
        "d = a/b # division\n",
        "e = a**b # exponentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KJJgT1xf71k"
      },
      "source": [
        "## Numpy\n",
        "\n",
        "Everything that we do in machine learning is linear algebra (matrix multiplication). Numpy is a python library specialized for operations on arrays (vectors, matrices and N-dimensional tensors). Numpy is widely used, and it is pre-installed on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db8xhu84f71k"
      },
      "outputs": [],
      "source": [
        "# import the library\n",
        "import numpy as np\n",
        "\n",
        "# create a 1D array (vector)\n",
        "array1 = np.array([1.1, 2.2, 3.3])\n",
        "print(array1)\n",
        "\n",
        "# create a 2D array (matrix)\n",
        "array2 = np.array([[1.1, 2.2],\n",
        "                   [3.3, 4.4]])\n",
        "print(array2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AurwwG3tf71l"
      },
      "outputs": [],
      "source": [
        "# check the shape of the arrays\n",
        "print(array1.shape)\n",
        "print(array2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWrSotx4f71l"
      },
      "outputs": [],
      "source": [
        "# perform some multiplications\n",
        "array3 = np.array([[2.3, 4.9],\n",
        "                  [3.2, 5.4],\n",
        "                  [1.8, 4.5]])\n",
        "\n",
        "result = array3 @ array2 # matrix multiplication\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-BASO0uf71m"
      },
      "outputs": [],
      "source": [
        "# access the value of the arrays or store values\n",
        "\n",
        "k1 = array1[0]\n",
        "print(k1)\n",
        "\n",
        "array1[2] = 0\n",
        "print(array1)\n",
        "\n",
        "k2 = array3[2, 1]\n",
        "print(k2)\n",
        "\n",
        "k3 = array3[:, 0]\n",
        "print(k3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo7GKe6sf71m"
      },
      "source": [
        "## Cycles and functions\n",
        "\n",
        "The command `for` is used to create a cycle, while the command `def` is used to define a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUHxTkfxf71n"
      },
      "outputs": [],
      "source": [
        "# For cycle\n",
        "for i in range(10):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uH5gN6af71o"
      },
      "outputs": [],
      "source": [
        "# Define functions\n",
        "\n",
        "def multip(a, b):\n",
        "    temp = a*b\n",
        "\n",
        "    return temp\n",
        "\n",
        "a = 10\n",
        "b = 3\n",
        "\n",
        "print(multip(a, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVNObfqzf71o"
      },
      "source": [
        "## Matplotlib\n",
        "\n",
        "Another useful python library is Matplotlib, which is used to visualize the data. Like Numpy, Matplotlib is widely used and it is generally pre-installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPpZc9aCf71o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0, 6*np.pi, 100) # this is used to create an array from 0 to 6pi containing 100 points regularly spaced\n",
        "y = np.sin(x) # compute the sine of x\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCL0yxy-f71p"
      },
      "source": [
        "## Exercise 1: toy dataset\n",
        "\n",
        "We want to test PCA on a toy dataset. The dataset contains 2 variables $x$ and $y$. The second variable is a linear function of the first one, but it is corrupted with Gaussian noise. PCA should be able to identify the direction of maximum variance (the first PC) and relegate the noise in the last PC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9p_jEu-f71p"
      },
      "outputs": [],
      "source": [
        "# create the x and y variables\n",
        "\n",
        "n_points = 100\n",
        "n_variables = 2\n",
        "\n",
        "x = np.linspace(0, 5, n_points)\n",
        "y_true = 3.2 + 2.2*x\n",
        "\n",
        "rng = np.random.default_rng(121421321)\n",
        "noise = rng.normal(loc=0, scale=0.75, size=n_points)\n",
        "y_noise = y_true + noise\n",
        "\n",
        "plt.plot(x, y_true, c='r', ls='--', label='true')\n",
        "plt.scatter(x, y_noise, c='k', alpha=0.3, label='noisy')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7CdvFXEf71p"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "D = np.zeros((n_points, n_variables)) # empty matrix\n",
        "\n",
        "# this operation is called slicing and it is used to put or extract values from an array\n",
        "D[:, 0] = x # put x in the first column (0) of the array\n",
        "D[:, 1] = y_noise\n",
        "\n",
        "print(D.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DXCequhf71p"
      },
      "source": [
        "## PCA algorithm\n",
        "\n",
        "To find the PCs, we need to follow these steps:\n",
        "- Center and scale the dataset:\n",
        "\\begin{equation}\n",
        "\\mathbf{D}_0 = (\\mathbf{D} - \\mathbf{D}_{\\mu})\\mathbf{D}^{-1}_{\\sigma}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "- Compute the covariance matrix:\n",
        "\\begin{equation}\n",
        "\\mathbf{K} = \\mathbf{D}_0^T\\mathbf{D}_0\n",
        "\\end{equation}\n",
        "\n",
        "- Compute the eigendecomposition of the covariance matrix:\n",
        "\\begin{equation}\n",
        "\\mathbf{K} = \\mathbf{A} \\mathbf{L} \\mathbf{A}^T\n",
        "\\end{equation}\n",
        "\n",
        "- Sort the eigenvectors in descending order of eigenvalues.\n",
        "\n",
        "- Transform the scaled dataset:\n",
        "\\begin{equation}\n",
        "\\mathbf{Z} = \\mathbf{D}_0 \\mathbf{A}\n",
        "\\end{equation}\n",
        "\n",
        "- The original dataset can be reconstructed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{D} = (\\mathbf{Z} \\mathbf{A}^T)\\mathbf{D}_{\\sigma} + \\mathbf{D}_{\\mu}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYehQ9vsf71p"
      },
      "outputs": [],
      "source": [
        "# To do: center and scale the dataset\n",
        "\n",
        "# Hints:\n",
        "# - Create two empty matrices D_m and D_s with the command np.zeros()\n",
        "# - Store the mean and the standard deviations in the matrix using a for loop\n",
        "# - Compute the mean and the standard deviation using np.mean() and np.std()\n",
        "# - Compute the inverse of D_s using np.linalg.inv()\n",
        "# - Use the command @ to multiply the matrices\n",
        "\n",
        "D_m = np.zeros_like(D)\n",
        "D_s = np.zeros((n_variables, n_variables))\n",
        "for i in range(n_variables):\n",
        "    D_m[:, i] = np.mean(D[:, i])\n",
        "    D_s[i, i] = np.std(D[:, i])\n",
        "\n",
        "D0 =  (D - D_m) @ np.linalg.inv(D_s)\n",
        "\n",
        "plt.scatter(D0[:, 0], D0[:, 1], c='k', alpha=0.5)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TIJzld5f71q"
      },
      "outputs": [],
      "source": [
        "# To do: compute the covariance matrix\n",
        "\n",
        "# Hint:\n",
        "# - Use the function np.transpose() to transpose the matrix\n",
        "\n",
        "K = D0.T @ D0\n",
        "print(K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1ItRi_Jf71q"
      },
      "outputs": [],
      "source": [
        "# To do: compute the eigendecomposition of the covariance matrix and sort the eigenvalues and eigenvectors in decreasing order\n",
        "\n",
        "# Hint:\n",
        "# - Use the function np.linalg.eig() to compute the eigendecomposition\n",
        "# - Use np.argsort() to sort the eigenvectors\n",
        "\n",
        "l, A = np.linalg.eig(K)\n",
        "print(l)\n",
        "index_sort = np.argsort(l)[::-1]\n",
        "\n",
        "l = l[index_sort]\n",
        "A = A[:, index_sort]\n",
        "\n",
        "L = np.diag(l)\n",
        "\n",
        "print(L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIIYuzD8f71q"
      },
      "outputs": [],
      "source": [
        "# To do: compute the transformed dataset\n",
        "\n",
        "Z = D0 @ A\n",
        "print(Z.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmASvmJwf71q"
      },
      "outputs": [],
      "source": [
        "# To do: reconstruct the dataset\n",
        "\n",
        "D_rec = (Z @ A.T) @ D_s + D_m\n",
        "\n",
        "plt.scatter(D[:, 0], D[:, 1], c='k', alpha=0.5, label='original')\n",
        "plt.scatter(D_rec[:, 0], D_rec[:, 1], c='r', alpha=0.5, label='reconstructed')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2BIqvm6f71q"
      },
      "outputs": [],
      "source": [
        "# To do:\n",
        "# - compress the dataset by removing the last PC,\n",
        "# - reconstruct the dataset with the compressed representation\n",
        "\n",
        "q = 1\n",
        "Aq = A[:, :q]\n",
        "Zq = Z[:, :q]\n",
        "\n",
        "D_rec = (Zq @ Aq.T) @ D_s + D_m\n",
        "\n",
        "plt.scatter(D[:, 0], D[:, 1], c='k', alpha=0.5, label='original')\n",
        "plt.scatter(D_rec[:, 0], D_rec[:, 1], c='r', alpha=0.5, label='reconstructed')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkvgEijIf71q"
      },
      "outputs": [],
      "source": [
        "# To do: create a python function that takes the data as input and computes the PCA, returning Z, L, A as output\n",
        "\n",
        "def pca(D):\n",
        "    _, n_variables = D.shape\n",
        "\n",
        "    D_m = np.zeros_like(D)\n",
        "    D_s = np.zeros((n_variables, n_variables))\n",
        "    for i in range(n_variables):\n",
        "        D_m[:, i] = np.mean(D[:, i])\n",
        "        D_s[i, i] = np.std(D[:, i])\n",
        "\n",
        "    D0 =  (D - D_m) @ np.linalg.inv(D_s)\n",
        "\n",
        "    K = D0.T @ D0\n",
        "\n",
        "    l, A = np.linalg.eig(K)\n",
        "    index_sort = np.argsort(l)[::-1]\n",
        "\n",
        "    l = l[index_sort]\n",
        "    A = A[:, index_sort]\n",
        "\n",
        "    L = np.diag(l)\n",
        "    Z = D0 @ A\n",
        "\n",
        "    return Z, L, A\n",
        "\n",
        "Z, L, A = pca(D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU0O-U3nf71r"
      },
      "source": [
        "## Singular value decomposition\n",
        "\n",
        "The PCA can be computed also by applying the SVD to the dataset:\n",
        "\\begin{equation}\n",
        "\\mathbf{D}_0 = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^T\n",
        "\\end{equation}\n",
        "\n",
        "in which:\n",
        "- $\\mathbf{A} = \\mathbf{V}$,\n",
        "- $\\mathbf{L} = \\boldsymbol{\\Sigma}^2$,\n",
        "- $\\mathbf{u}_i = \\frac{\\mathbf{z}_i}{||\\mathbf{z}_i||}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RJWCU5Ff71r"
      },
      "outputs": [],
      "source": [
        "# To do: check if this is true\n",
        "\n",
        "# Hint:\n",
        "# - Use np.linalg.svd(D0, full_matrices=False) to compute the svd\n",
        "\n",
        "U, s, Vt = np.linalg.svd(D0, full_matrices=False)\n",
        "V = Vt.T\n",
        "\n",
        "print(A)\n",
        "print(V)\n",
        "\n",
        "print(L)\n",
        "print(np.diag(s)**2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB6ulwKSf71r"
      },
      "source": [
        "## Scikit-learn\n",
        "\n",
        "Scikit-learn is a python library that contains many machine-learning models, including PCA. It can also handle centering and scaling more naturally. However, we need to install it first because it is not included in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2qp2eOPf71r"
      },
      "outputs": [],
      "source": [
        "! pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38gAUk-Xf71r"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# To do: use sklearn to compute PCA\n",
        "\n",
        "# Hint:\n",
        "# - Use StandardScaler to center and scale the data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "D0 = scaler.fit_transform(D)\n",
        "\n",
        "pca = PCA(n_components=1)\n",
        "pca.fit(D0)\n",
        "\n",
        "A = pca.components_.T\n",
        "Z = D0 @ A\n",
        "ex_var = pca.explained_variance_ratio_\n",
        "\n",
        "print(ex_var)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH5LAZYqf71r"
      },
      "source": [
        "## A more complex example\n",
        "\n",
        "We now want to apply PCA to the \"wine dataset\". This dataset contains 178 samples with 13 different features. All the wines are categorized in 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK_tF2Nef71s"
      },
      "outputs": [],
      "source": [
        "# Import the dataset\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine_data = load_wine().data\n",
        "n_samples, n_features = wine_data.shape\n",
        "\n",
        "wine_features = load_wine().feature_names\n",
        "wine_classes = load_wine().target\n",
        "\n",
        "print(wine_data.shape)\n",
        "print(wine_features)\n",
        "print(wine_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If6Ie7bMf71s"
      },
      "outputs": [],
      "source": [
        "# To do: visualize the dataset\n",
        "\n",
        "# Hint:\n",
        "# - Use plt.scatter() to plot the distribution of 'alcohol' and 'malic_acid'\n",
        "# - Color the scatter using wine_classes\n",
        "\n",
        "index_1 = wine_features.index('alcohol')\n",
        "index_2 = wine_features.index('malic_acid')\n",
        "plt.scatter(wine_data[:, index_1], wine_data[:, index_2], c=wine_classes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPLC_Cdhf71s"
      },
      "outputs": [],
      "source": [
        "# To do: compute the PCA and show the distribution of the explained variance\n",
        "\n",
        "# Hint:\n",
        "# - Use pca.explained_variance_ratio_ to get the explained variance\n",
        "# - Use np.cumsum() to compute the cumulative variance\n",
        "\n",
        "scaler = StandardScaler()\n",
        "D0 = scaler.fit_transform(wine_data)\n",
        "\n",
        "pca = PCA(n_components=n_features)\n",
        "pca.fit(D0)\n",
        "\n",
        "A = pca.components_.T\n",
        "Z = D0 @ A\n",
        "ex_var = pca.explained_variance_ratio_\n",
        "cum_ex_var = np.cumsum(ex_var)\n",
        "\n",
        "plt.scatter(1 + np.arange(n_features), ex_var, c='k', label='expl. var.')\n",
        "plt.scatter(1 + np.arange(n_features), cum_ex_var, c='r', label='cum. expl. var.')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNdR1IFuf71s"
      },
      "outputs": [],
      "source": [
        "# To do: visualize the first 2 PCs and color them by the wine class\n",
        "\n",
        "plt.scatter(Z[:, 0], Z[:, 1], c=wine_classes)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nfDiuVuf71s"
      },
      "outputs": [],
      "source": [
        "# To do: visualise the scores of the PCs\n",
        "\n",
        "# Hint:\n",
        "# - Use plt.bar() to plot a bar plot\n",
        "\n",
        "plt.bar(np.arange(n_features), A[:, 0])\n",
        "plt.xticks(np.arange(n_features), wine_features, rotation=90)\n",
        "plt.ylabel('1 eigenvector weight')\n",
        "plt.show()\n",
        "\n",
        "plt.bar(np.arange(n_features), A[:, 1])\n",
        "plt.xticks(np.arange(n_features), wine_features, rotation=90)\n",
        "plt.ylabel('2 eigenvector weight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct6DpgqXf71s"
      },
      "source": [
        "## Outlier detection\n",
        "\n",
        "We can use PCA to detect probable outliers in a dataset. We consider a toy dataset containing 2 features ($x$ and $y$) and 100 samples. For the first 95 samples, $x$ is in the range $[0, 10]$ and $y = 4x + 2 + \\epsilon$, where $\\epsilon$ is Gaussian noise with $\\mu = 0 $ and $\\sigma = 2.5$. The last 5 points are outliers with $x_{\\mathrm{out}}$ is in the range $[2, 3]$ and $y_{\\mathrm{out}} = 3 x^2 + x + 10 + \\epsilon_{\\mathrm{out}}$, where $\\epsilon_{\\mathrm{out}}$ is Gaussian noise with $\\mu_{\\mathrm{out}} = 0 $ and $\\sigma_{\\mathrm{out}} = 5$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFYK2xblf71s"
      },
      "outputs": [],
      "source": [
        "# First we import the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "\n",
        "n_points = 95 # number of points in the dataset\n",
        "x = np.linspace(0, 10, n_points) # the function linspace creates a vector of 95 values in the range [0,10]\n",
        "\n",
        "# this command is used to set a fixed seed for random sampling,\n",
        "# so that we have the same result when re-running the code.\n",
        "np.random.seed(1)\n",
        "\n",
        "noise = np.random.normal(0, 2.5, size=n_points) # a vector of noise\n",
        "y = 4*x + 2 + noise\n",
        "\n",
        "n_outl = 5\n",
        "x_outl = np.linspace(2, 3, n_outl)\n",
        "noise_outl = np.random.normal(0, 5, size=n_outl)\n",
        "y_outl = 3*x_outl**2 + x_outl + 10 + noise_outl\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.scatter(x, y, c='k', label='correct')\n",
        "ax.scatter(x_outl, y_outl, c='r', label='outliers')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5snPYUof71t"
      },
      "outputs": [],
      "source": [
        "D = np.empty((n_points + n_outl, 2)) # create an empty matrix of size (n_points + n_outl, 2)\n",
        "\n",
        "# the function concatenate is used to concatenate the x and y arrays and\n",
        "# store them in the D matrix\n",
        "D[:,0] = np.concatenate([x, x_outl])\n",
        "D[:,1] = np.concatenate([y, y_outl])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.scatter(D[:,0], D[:,1], c='k')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehkZBPpBf71t"
      },
      "outputs": [],
      "source": [
        "# Compute PCA and plot the transformed dataset\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "D0 = scaler.fit_transform(D)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(D0)\n",
        "\n",
        "A = pca.components_.T\n",
        "Z = D0 @ A\n",
        "lamda = pca.singular_values_**2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.scatter(Z[:,0], Z[:,1], c='k')\n",
        "ax.set_xlabel('PC 1')\n",
        "ax.set_ylabel('PC 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTe5dkznf71u"
      },
      "source": [
        "To find the outliers, we build a classifier using the 2nd PC. To do that, we compute the empirical cumulative distribution function (ecdf) of PC scores squared divided by the corresponding eigenvalue (Mahalanobis distance):\n",
        "\n",
        "\\begin{equation}\n",
        "d_{M, i} = \\frac{z^2_{i,2}}{l_2}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathrm{edfc}(t) = \\sum_{j = 1}^k d_{M, j} < t\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhjpDbqJf71u"
      },
      "outputs": [],
      "source": [
        "def ecdf(pc):\n",
        "    i_sort = np.argsort(pc)\n",
        "    pc_sort = pc[i_sort]\n",
        "    dist_sort = np.arange(1, pc_sort.size+1)/pc_sort.size\n",
        "    return i_sort, pc_sort, dist_sort\n",
        "\n",
        "# We calculate the Mahalanobis distance using PC2\n",
        "dm_pc2 = Z[:,1]**2/lamda[1]\n",
        "i_sort, pc2_sort, dist_sort = ecdf(dm_pc2)\n",
        "threshold = 0.95\n",
        "\n",
        "# We plot the cumulative distribution\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.scatter(pc2_sort, dist_sort, c='k', s=10)\n",
        "ax.set_xlabel('Dm for PC 2')\n",
        "ax.set_ylabel('Distribution')\n",
        "ax.hlines(threshold, pc2_sort.min(), pc2_sort.max(), color='r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zjB_J0ef71u"
      },
      "outputs": [],
      "source": [
        "mask = dist_sort > threshold  # this mask operation creates a boolean array that follows the conditions we impose\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.scatter(D[i_sort[~mask], 0], D[i_sort[~mask], 1], c='k', label='correct')\n",
        "ax.scatter(D[i_sort[mask], 0], D[i_sort[mask], 1], c='r', label='outliers')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_ml2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}